好的，这篇学术论文的完整中文翻译如下：

---

# LinnOS：使用轻量级神经网络在不可预测的闪存上实现可预测性

**作者：** Mingzhe Hao, Levent Toksoz, Nanqiqin Li, Edward Edberg Halimi, Henry Hoffmann, and Haryadi S. Gunawi

**单位：** 芝加哥大学，苏里亚大学

# 摘要

本文介绍了 LinnOS，这是一个利用轻量级神经网络在极细粒度（每次 I/O）上推断 SSD 性能的操作系统，并帮助并行存储应用实现性能的可预测性。LinnOS 支持黑盒设备和真实的生产环境踪迹，无需用户提供任何额外输入，同时其性能优于工业界的机制和其他方法。我们的评估表明，与对冲（hedging）和基于启发式的方法相比，LinnOS 将平均 I/O 延迟降低了 $9.6\% - 79.6\%$，推理准确率达到 $87\% - 97\%$，每次 I/O 的推理开销仅为 $4 - 6\mu s$，这表明将机器学习融入操作系统以进行实时决策是可行的。

# 1 引言

可预测的性能是当今及未来系统的一项重要需求。对于为网络搜索、电子邮件和许多其他类型交互式服务提供支持的数据中心系统而言，可预测的延迟甚至更为重要。好的一面是，速度越来越快的 SSD 已经面市，并正成为存储市场的主导因素。坏的一面是，SSD 内部的复杂性持续增长，在现代闪存设备上实现高度可预测的延迟仍然是一个悬而未决的挑战性问题。

由于固有的 NAND 特性，现代闪存设备的行为就像一个“操作系统”，通过垃圾回收（garbage collection）、缓冲区刷新、磨损均衡和读取修复等后台操作来管理其所有内部资源。虽然这些操作很重要，但它们对延迟的可预测性构成了威胁，这在近年来仍是许多存储行业面临的新问题。此外，一份报告指出，对于某些在线应用，闪存设备贡献了超过 $19\%$ 的总响应时间，因此应该探索更多的解决方案。

由于设备本身无法掩盖不可预测的延迟，大量的研究都投入到了这个领域。“白盒”方法——即重新设计设备内部结构——功能强大，但除非 SSD 供应商实施这些建议，否则其采用门槛很高。处于中间地带的“灰盒”方法建议进行部分的设备级修改，并结合操作系统或应用级的变更来共同抑制延迟的不可预测性。然而，它们也依赖于供应商修改设备接口的意愿。最后，更易于采纳的“黑盒”技术试图在不修改底层硬件及其抽象层次的情况下掩盖不可预测性。其中一些技术专门为 SSD 使用优化了文件系统或存储应用，而另一些则简单地使用推测执行（speculative execution），但由于对存储行为一无所知，付出了额外 I/O 的代价。在以上所有方法中，鉴于其简单性和能够缓解每一次慢速 I/O 的能力，推测执行可以说是最流行的解决方案。例如，作为推测执行的一种形式，“对冲请求”（hedged requests） 在当今许多广泛使用的键值存储中都得到了支持。

我们采取一种新方法：让设备保持其黑盒特性，并且不重新设计文件系统或应用程序，而是去学习设备的行为（即，不做到存储无知）。我们方法的关键是学习。我们能否以黑盒方式学习底层设备的行为，并利用学习结果来提高可预测性，从而使应用程序能够提前知道其性能期望是否能被满足？这正是机器学习可能有所帮助的领域。我们引入 LinnOS，一个能够使用轻量级神经网络学习和推断每次 I/O 速度的操作系统，具有高准确率和最小的开销。我们展示了 LinnOS 如何帮助存储应用程序，特别是有内置故障转移逻辑的存储阵列/集群（例如，闪存 RAID、Cassandra、MongoDB），在不可预测的闪存上实现极高的延迟可预测性。

LinnOS 面临的最大挑战是要像流行的推测执行方法一样有效和细粒度，后者可以通过向另一个节点或设备发送重复的 I/O 来缓解每一次慢速 I/O。推测执行在提高可预测性方面的成功是以牺牲资源利用率为代价的。避免这种代价的关键在于了解设备内部正在进行的活动，并始终将 I/O 调度到那些能提供更快响应的设备上。

然而，由于保持抽象屏障是一项基本约束，我们需要学习推断延迟并使推断结果高度可用。实现这一点需要在非常精细的、每次 I/O 的尺度上进行实时学习和推断。据我们所知，由于在实现每次 I/O 的准确性和快速在线推理方面存在挑战，目前还没有现成的 I/O 调度学习方法支持如此细粒度的学习。为了解决这个问题，LinnOS 引入了三项技术贡献。

首先，LinnOS 将困难的延迟推断问题转化为一个简单的二元推断（“快”或“慢”速）。我们利用了系统部署中典型的延迟分布，特别是形成具有高 alpha 值的帕累托分布的行为。换句话说，大多数时候（例如，>90%），延迟非常稳定，但偶尔（例如，<10% 的时间），延迟会表现出长尾行为。闪存存储的行为也反映了同样的分布。在这种简单的视角下，用户只希望“慢”的 I/O 变得“快”，推断确切的延迟是多此一举。基于这一直觉，LinnOS 配备了一种算法，可以监控当前在闪存设备上运行的工作负载的延迟分布，并计算出一个大致最优的阈值来区分快慢速度范围。

其次，利用二元模型，LinnOS 对集群存储应用实施了一种简单的准入控制。LinnOS 对每个传入的 I/O 使用一个轻量级神经网络模型进行二元推断，该模型以黑盒方式提前推断 I/O 速度，无需设备或应用的任何指导。如果 I/O 被推断为快，LinnOS 将其提交给闪存设备；否则，它将撤销该 I/O 并通知应用程序。有了这个及时而直接的二元信息，存储应用程序可以快速地将 I/O 故障转移到持有相同副本的另一个节点或设备。此外，由于原始的慢速 I/O 已被撤销，资源得到了有效利用。

第三，LinnOS 平衡了神经网络的准确性和性能。高准确性但高推理时间会导致显著的单次 I/O 开销，特别是对于现代 SSD。另一方面，通过降低准确性来降低推理时间会导致许多错误的推断，使得存储性能难以分析。

为实现高准确性，LinnOS 分析提交到设备的数百万次 I/O 的延迟（一个天然的“数据湖”），这些数据将用于训练神经网络。此外，由于我们将回归问题转化为简单的二元分类，输出准确性显著提高（类似于“猫或狗”图像分类的简单性）。下一个挑战是决定哪些输入特征对提高准确性最重要。我们将介绍我们的惊人发现。例如，“看起来很重要”的特征，如块偏移、读/写标志或长期的写历史记录，并不起重要作用。最终，输入特征变得易于处理，仅包含两类信息：最近完成的少数几次 I/O 的延迟，以及这些 I/O 和当前待推断的 I/O 到达时挂起的 I/O 数量。

![](https://cdn-mineru.openxlab.org.cn/result/2025-07-07/6c1e16d3-ec8f-454c-b865-5fc71eb2fec6/9f7281c389ed3ddeba3deb2a58f22e690a6e505b01b1b70b16202fd69cde3338.jpg)  
**图 1: 延迟分布。** 这些图表显示了块级读取延迟的累积分布函数（CDF），如第 2 节所述。对于左图，我们在五种不同的 SSD 型号上运行了一个 FIO 工作负载（五条 CDF 线）。对于右图，我们绘制了从 Azure、Bing 和 Cosmos 集群中的四个读写服务器（彩色线）和两个只读服务器（粗灰线）获得的七个块级踪迹的延迟。x 轴因商业原因已作匿名化处理。这些踪迹可通过与微软签订保密协议获得。

为了性能，挑战在于在亚 $10\mu s$ 的时间内完成一次推断（准入决策），这对于我们针对快速存储设备的细粒度实时推断至关重要。虽然使用更深的模型和更多特征可以提高准确性，但这会损害推断延迟，并且对于在 I/O 层中使用来说成本太高。通过几次设计迭代，我们通过多种方法将推断时间缩短到 $4 - 6\mu s$，同时只有微小的准确性损失：一个 3 层的轻量级神经网络、权重 quantization（量化），以及（可选的）2 线程/2 核矩阵乘法。

我们的评估表明，LinnOS 支持各种各样的黑盒设备（测试了 10 种设备型号），并能在真实的生产踪迹上工作，无需用户提供任何额外输入（例如，关于踪迹/设备或延迟期限的提示等），其性能优于纯对冲等工业界方法，并击败了我们设计的简单和“高级”启发式方法。与这些方法相比，LinnOS 在学习结果的基础上辅以对冲，进一步将平均 I/O 延迟降低了 $9.6 - 79.6\%$，准确率达到 $87 - 97\%$，每次 I/O 的推断开销仅为 $4 - 6\mu s$。

总的来说，我们表明，采用机器学习方法让操作系统学习黑盒设备是可行的。我们在结尾提出了许多未来值得探索的有趣讨论。LinnOS 的代码已公开。

# 2 背景

**不可预测性。** 为了引出问题，图 1a 中的彩色线条显示了在五种不同的 SSD 型号上运行读写工作负载时的读取延迟分布，这些 SSD 型号从消费级 SATA 和 NVMe SSD 到新型的数据中心 SSD 不等。型号 A 在大约“p98”（第 98 百分位数）之前提供了快速且稳定的延迟，但型号 B 和 C 分别从 p90 和 p75 开始表现出更长的延迟尾部。然而，当写操作被转换为读 I/O 时，性能变得高度稳定，没有太多的延迟尾部（图中未显示）。图 1b 也在微软基于 SSD 的服务器的真实生产场景中证实了这一点。彩色线条显示了读写服务器的块级读取延迟（变异性更大），灰色线条则代表只读服务器（可预测性更高）。所有这些都证实了由写入触发的垃圾回收（GC）、缓冲区刷新和其他内部操作是如何与用户读取 I/O 竞争的。我们只解决读取性能的不可预测性问题，因为我们发现写入延迟（令人惊讶地）是稳定的，因为它们被设备上的内部内存缓冲区吸收，因此不受内部竞争的影响。写入延迟尖峰仅在缓冲区满时发生（由于内部定期刷新，这种情况很少发生）。

**内部复杂性。** 推断闪存驱动器何时出现尾部延迟是困难的，因为内部复杂性是影响延迟行为的因素。举几个例子，如果 I/O 落入同一个芯片或通道，它们会相互竞争，这取决于隐藏的条带化和分区逻辑；两个进入不同通道的用户 I/O 可能会有不同的命运，当一个通道被通道内芯片之间的 GC 数据传输占用时。我们的内部发现表明，SSD 可以有宽布局（例如，32 个通道，每个通道 4 个芯片）或深布局（例如，4 个通道，每个通道 16 个芯片），后者会导致更多的通道竞争。一些 SSD 采用从 256MB 到小至 $12\mathrm{MB}$ 的大型写缓冲区，并且可以从每 8ms 到高达一秒的周期内定期刷新。如图 1 所示，这种内部竞争可以影响从 $1\%$ 到 $25\%$ 的所有读取请求。

在这种背景下，现代存储应用程序通常采用一种“等待后推测”的方法，这种方法对设备的内部复杂性一无所知。例如，通过对冲，应用程序等待一个超时（例如，p95 延迟），然后发出额外的推测性 I/O，并使用响应更快的结果。推测执行对粗粒度任务（几十到几百秒）效果很好，但对于闪存存储来说效果不佳，因为当期望的响应时间小于几毫秒时，等待的代价很高（§5.3）。

**机器学习。** 在我们尝试神经网络等机器学习技术之前，我们曾问过，简单的启发式方法是否足以准确推断每次 I/O 的速度。例如，有人可能会假设一个长的 I/O 队列长度意味着更长的延迟——这种启发式方法对旋转磁盘效果很好。然而，对于 SSD，由于内部的复杂性，队列长度与延迟没有高度相关性（我们没有发现队列长度和 I/O 延迟之间有很高的皮尔逊相关性或斯皮尔曼相关性）。我们还创建了一个更“高级”的启发式方法，但它没有产生令人满意的结果（详见评估部分）。虽然可以继续精心设计能够适应不同工作负载和设备型号的正确启发式方法，但我们决定求助于机器学习。最近的操作系统和分布式系统研究成功地将机器学习用于资源分配和调度。针对 I/O 层的类似探索可以带来强大的结果，正如我们在本文中所示。

![](https://cdn-mineru.openxlab.org.cn/result/2025-07-07/6c1e16d3-ec8f-454c-b865-5fc71eb2fec6/6a9e27bb4eb38e478622487469c1b0728843a799a5df208d271b3650489e138d.jpg)  
**图 2: 使用场景。** 该使用场景在 3.1 节中解释。"LC" 表示延迟关键。

# 3 概述

我们现在给出 LinnOS 的概述，包括其使用场景、架构和挑战，然后是其设计（§4）。

# 3.1 使用场景

LinnOS 对于并行、冗余的存储系统（如维护同一数据块多个副本的闪存阵列，无论是基于集群还是 RAID）非常有益，如图 2 所示。(a) 使用 LinnOS，当存储应用程序通过操作系统调用执行 I/O 时，可以添加一个一位的标志，提示 LinnOS 该 I/O 是延迟关键的（LC=true），例如用于交互式服务。这种对关键操作的标记已被多次提出，但在我们的案例中，该位用于触发 LinnOS 推断 I/O 延迟。(b) 在将 I/O 提交给底层 SSD 之前，LinnOS 将 I/O 信息输入到它已经训练好的神经网络模型中，该模型将做出一个二元推断：快或慢。(c) 如果输出是“快”，LinnOS 将 I/O 提交给设备。(d) 否则，如果推断为“慢”，LinnOS 将撤销该 I/O（不进入设备队列）并返回一个“慢”的错误码。(e) 收到错误码后，存储应用程序可以将相同的 I/O 故障转移到另一个副本。(f) 在最坏的情况下，应用程序必须故障转移到最后一个副本，这最后一次重试将不会被标记为延迟关键，以便 I/O 能够完成而不会被撤销。

# 3.2 整体架构

图 3 显示了 LinnOS 的整体架构，它由五个主要组件组成。

(a) **模型。** LinnOS 的核心是具有轻量级神经网络的快速推理模型（第 4.3 节）。模型的输入特征是关于当前未完成 I/O 和最近完成 I/O 的信息。该模型独立推断每个传入 I/O 的速度。模型的输出是关于 I/O 的二元推断（快/慢）。

(b) **追踪。** 为了训练模型，LinnOS 使用 SSD 当前正在服务的实时工作负载。为了获得丰富的代表性数据集，这可以在正常的繁忙时段进行。使用 blktrace 记录 I/O 元数据（块偏移、大小、读/写）及其产生的延迟。收集了数百万个 I/O 后，这自然形成了我们模型的“数据湖”。训练数据（收集的踪迹）预期与“测试数据”（模型激活时将被推断的 I/O）不同。

(c) **使用拐点分析进行标记。** 收集到的踪迹随后被提供给 LinnApp，一个支持性的用户级应用程序。LinnApp 有三个主要工作：标记、训练和将训练好的权重上传到 LinnOS。因为模型被设计为产生二元输出，所以模型必须用“快”和“慢”两个标签进行训练。因此，给定踪迹中的延迟分布，LinnApp 运行一个算法（§4.2.1）来找到“拐点”，这是一个划分快慢延迟范围的延迟值。

(d) **训练。** 有了这个拐点，LinnApp 用“快”和“慢”的标签来标记被追踪的 I/O，并进入训练阶段（使用 TensorFlow）。我们强调，标记是自动完成的，无需人工输入。这个训练阶段可以在任何地方运行，无论是 GPU 还是 CPU 节点。

(e) **上传权重。** 训练阶段为模型中的神经元生成权重，这些权重将被上传到 LinnOS。由于在操作系统内核中不太支持使用浮点数，权重通过量化转换为整数。然后模型被激活，LinnOS 准备好进行推断并撤销“慢”的 I/O。

# 3.3 挑战

使用机器学习方法对 I/O 速度进行在线、细粒度的推断，需要我们解决以下基本挑战。

![](https://cdn-mineru.openxlab.org.cn/result/2025-07-07/6c1e16d3-ec8f-454c-b865-5fc71eb2fec6/893767bee79e664749808b860db3a0909eaf3b399e49f6e0abccfda5323cf611.jpg)  
**图 3: LinnOS 架构。** 该图展示了 LinnOS 的架构，包括 LinnApp，如第 3.2 节所述。两个 SSD 图片代表同一个 SSD 实例；左边描述了追踪/训练，右边描述了在该 SSD 上的实时推理。

**高准确性。** 推理必须准确。我们不应该撤销可以快速服务的 I/O（“错误撤销”）或提交那些会变慢的 I/O（“错误提交”）。准确性取决于仔细的输出标记和输入特征选择。如果标签分类过于复杂，高准确性就很难实现，例如，我们发现按线性分桶（0-10, 10-20μs 等）或指数分桶（0-1, 2-4μs 等）进行分类很难做到准确，应作为未来工作。然而，简单的两类方法（快或慢）将输出简化为二进制格式，这有助于模型实现高准确性。

**快速推理。** 对于现代 SSD，虽然宣传的原始 NAND 读取延迟低于 100μs，但我们看到，对于数据中心 SSD 上的典型生产工作负载（第 5 节），实际用户感知的延迟在 50% 以上的时间里超过 200μs。鉴于此观察，我们认为挑战在于在 5μs 左右做出决策，这对于每次 I/O 来说是 <3% 的开销。快速推理取决于输入预处理、层的深度、神经元复杂度和特征表示。使用倾向于提高准确性的深层网络在我们的问题领域并不具吸引力。输入特征必须最小化，只包括那些重要的特征。因此，我们必须在准确性和性能之间取得平衡。此外，考虑到操作系统在 CPU 上运行，模型必须是 CPU 友好的。

**预见异构性。** 在闪存阵列（RAID 或基于集群）中，用户负载并不总是均衡的，而且所有的闪存硬件也可能不是同构的。因为这种异构性可能导致在不同设备上观察到不同的延迟分布，我们不应该使用一个全局的延迟值（例如，1ms 的拐点）来区分所有设备的快慢速度。例如，3ms 在较慢的 SSD 或负载较重的 SSD 上可能被认为是足够快的。虽然我们不期望异构性会是极端的（例如，一个好的存储系统通常能很好地平衡负载），但解决异构性仍然很重要。因此，LinnApp 收集每个设备的踪迹，并为阵列中的每个负载-设备对训练模型（图 4）。训练阶段完成后，LinnApp 将模型权重提供给基于集群阵列中的所有 LinnOS 实例，或者在基于 RAID 的阵列中提供给一个 LinnOS 实例。在后者中，LinnOS 为 RAID 中的 N 个驱动器携带 N 个训练好的模型。此外，为了预见工作负载随时间的变化，LinnApp 会偶尔重新收集踪迹（例如，每隔几小时）来检查拐点是否发生了显著变化，以至于必须重新训练模型。

![](https://cdn-mineru.openxlab.org.cn/result/2025-07-07/6c1e16d3-ec8f-454c-b865-5fc71eb2fec6/c696c4b89fdf77f775eecfdefafe823a3374afb361f0acb180bac8ef7df8043a.jpg)  
**图 4: 预见异构性。** 该图显示了异构的训练模型，如第 3.3 节所述。

# 4 LinnOS 设计

在本节中，我们描述我们对上述挑战的解决方案。据我们所知，LinnOS 是第一个能够以快速、准确、实时、细粒度和通用的方式成功推断 I/O 速度的操作系统。其关键在于 LinnOS 采用的神经网络模型的“轻量级”特性。本节按时间顺序介绍最终设计及其背后的主要直觉：从数据收集（§4.1）、通过拐点分析进行标记（§4.2）、模型设计（§4.3），到如何提高其准确性（§4.4）和性能（§4.5），并总结其优势（§4.6）。

# 4.1 训练数据收集

这个项目始于一个简单的问题：我们能否准确地推断每个 I/O 的性能？由于我们使用机器学习，准确性取决于可用的真实信号数据的数量，越多越好。幸运的是，I/O 系统天生就能收集大量数据。鉴于低开销的追踪工具和现代 SSD 每秒可以处理数十万次 I/O 的工作负载，为训练收集大量数据不是问题（一个庞大的“I/O 数据湖”）。

对于要建模的每个负载-SSD 对，LinnApp 收集在驱动器上运行的真实工作负载的踪迹。例如，为了推断部署中某个特定 SSD 上的生产工作负载性能，将收集在线踪迹。对于每个 I/O，我们收集五个原始字段：提交时间、块偏移、块大小、读/写，以及最重要的 I/O 完成时间。因为模型输入（第 4.3 节）不一定采用相同的原始字段，所以在此阶段，我们还将这些字段转换为输入特征格式。

这里的主要挑战是决定踪迹应该有多长。如果训练数据的行为（延迟分布）与“测试”数据（待推断的 I/O）的行为非常不同，推断准确性将会下降。在这项工作中，我们采用一种简单的方法，即使用一个繁忙时段的踪迹（例如，中午）。在评估中（§5.2），我们表明，对于生产工作负载，一个繁忙时段的踪迹很好地代表了其他时段，即拐点没有太大偏离。如上所述，为了预见工作负载行为的急剧变化，可以进行重新追踪和重新训练。

# 4.2 标记（使用拐点）

由于我们采用监督分类方法，模型必须使用标签进行训练。如果我们用实际的微秒级延迟来标记每个 I/O，那么对于我们的问题领域来说，标签会太多；用户可能不在乎 I/O 是否延迟了 $1\mu s$。另一个选择是使用线性（$0-10\mu s, 10-20\mu s$, 等等）或指数标记（$2-4\mu s, 4-8\mu s$, 等等）。虽然这些更合适，但在多次设计迭代后，模型仍然难以做到准确和快速。准确率仅达到 $60-70\%$，因为很多时候，一个本应属于特定组（例如，$128-256\mu s$）的 I/O 经常被错误地推断到相邻的组（例如，$256-512\mu s$）——“一只拉萨犬很容易被误认为是西施犬”。这也许是为什么之前在自动学习存储性能方面的成功都只在粗粒度级别上完成，例如对许多请求聚合的平均延迟或吞吐量。

![](https://cdn-mineru.openxlab.org.cn/result/2025-07-07/6c1e16d3-ec8f-454c-b865-5fc71eb2fec6/67bdedb9357ee3677c3aedae0fea0e58cee40fe41a7b0280de4e1ff2c17288f4.jpg)  
**图 5: 拐点（快/慢阈值）。** 这些图显示了使用较高、较低或半最优拐点（IP）作为快/慢阈值的结果，如第 4.2 节所述。图表格式为延迟 CDF，与图 1 相同。

考虑到所有这些，并理解了现场性能变化的行为方式，我们观察到延迟通常形成具有高 alpha 值的帕累托分布。如图 5a 所示，90% 的时间里，延迟可能很稳定，但在另外 10% 的时间里，它开始形成长尾。这种帕累托分布清晰地对比了快速和慢速区域。因此，可以做出一个简单的推测，即用户只担心尾部行为，而不是精确的延迟。

为了区分这两个区域，我们需要找到“最佳”拐点（在图 5a 中标记为 $\mathbf{IP}=? $）以最大化延迟减少。将拐点设置得过于宽松（例如，图 5b 中的 p95 延迟）会使 LinnOS 将 p90 和 p95 之间相对较慢的 I/O 视为“快”（不进行故障转移），从而减少了有效重试的范围，因此未能削减许多尾部延迟，如图 5b 中原始分布和预测分布之间的小阴影区域所示（详见 §4.2.1）。另一方面，将拐点设置得过低（例如，图 5c 中的 p80 延迟）会使 LinnOS 撤销过多的 I/O，包括那些本应是快速的，这将导致不必要的重试开销，如图 5e 所示。

一个最优的拐点意味着，对于每个将被撤销的慢速 I/O，其他副本很可能在同一时间范围内快速地为它服务。同样，对于每个快速的 I/O，它不应该被故障转移。找到这个最优点将使原始的重尾分布和无尾分布之间的差距最大化，如图 5d 中的大阴影区域所示。然而，在实践中找到一个最优值是困难的，根本原因在于许多未知数：我们不知道请求将被故障转移到哪个副本（取决于应用程序）；训练数据只是对未来未知测试数据的近似；CPU 或网络拥塞等其他可变性也可能成为未知的重试开销因素。下一节将描述我们在为每个工作负载-设备对寻找半最优拐点方面的尽力而为的算法。

# 4.2.1 拐点算法

首先，在数据收集期间，我们收集了在 $d$ 个设备 $D_{1}$ 到 $D_{d}$ 上运行的 $t$ 个工作负载踪迹 $T_{1}$ 到 $T_{t}$，其中 $t=d$。每个踪迹 $T_{i}$ 都为我们提供了在设备上运行的工作负载的延迟分布（如图 5a）。为了为每个 $T_{i}-D_{i}$ 对找到唯一的拐点（IP）值，我们基于随机副本选择进行用户空间模拟，假设延迟在各个 SSD 之间是独立的。为便于说明，我们在下面的解释中使用特定的设备编号（例如，$D_i$）。

(1) 对于每个 $T_i-D_i$ 对，我们选择一个起始 IP 值，该值位于 CDF 斜率为 1 的地方（可能进入尾部区域）。例如，如果对于 $D_1$，$T_1$ 的 45 度斜率位于 $y=p90.5$ 且 $x=1ms$，那么 IP 值初始设置为 1ms。
(2) 对于当前模拟的设备 $D_1$，我们模拟一百万个 I/O $(r_{i=0..1000000})$，其中每个 I/O 请求 $r_i$ 从 $T_1$ 的真实延迟分布中取一个随机延迟值。然后我们模拟 LinnOS 的准入控制：如果选择的延迟小于 1ms（当前 IP），则 $r_i$ 的新延迟设置为相同的值；否则，如果大于 1ms，它将被撤销并故障转移到另一个随机选择的节点（例如，$D_4$），从其踪迹 $T_4$ 中选取一个新的随机延迟，并重复准入控制（提交或撤销）。我们假设有三个副本（可配置），因此一个请求最多只能被撤销两次。
(3) 该模拟为工作负载踪迹 $T_1$ 中的所有 $r_i$ 产生新的、优化后的延迟，这些 $r_i$ 之前只去往一个设备 $D_1$，但现在可以像激活了 LinnOS 准入控制一样被重定向。这些优化后的 $r_i$ 延迟构成了新的 CDF（如图 5d 中的粗蓝线）。使用原始 CDF 和新 CDF，我们可以计算面积差异（图 5d 中的阴影“提升区域”），这代表了如果使用 1ms ($p90.5$) 作为 IP 值所带来的延迟增益。
(4) 仍然对于 $D_1$，我们通过在初始 IP 值的 $\pm10$ 百分位数范围内移动 $\pm0.1$ 百分位数来重复上述所有步骤。对于每个新的 IP 值，模拟都会给出一个新的提升区域。我们现在可以选择 $IP^{max}$，即给出最大（正）提升区域的 IP 值，该值将用作训练设备 $D_1$ 模型的快慢阈值。
(5) 我们对其他设备（$D_2, D_3$ 等）重复所有步骤。最后，对于每个 $T_i-D_i$ 对，我们的算法都会生成一个唯一的 $IP_i^{max}$ 值。所有这些步骤在重新校准时都会重复（§4.4）。

# 4.3 轻量级神经网络模型

在我们决定构建一个轻量级神经网络模型之前，我们探索了各种学习方法，如逻辑回归、决策树和随机森林。我们发现准确率仅在 $17-84\%$ 之间，而一个基础的神经网络可以达到更好的准确率。尽管可以继续优化这些方法中的每一种以发挥其全部潜力，但我们决定从我们初始神经网络模型提供的可接受基线开始。下面，我们描述我们的最终模型（图 6），从输入特征、它们的表示到神经层。我们将强调我们如何利用存储直觉来设计模型，而不是采用暴力破解的方法。

![](https://cdn-mineru.openxlab.org.cn/result/2025-07-07/6c1e16d3-ec8f-454c-b865-5fc71eb2fec6/d8c3ea4db1d01a9fb19498d7b12fe48124eb2e54f41c73d057af43cb319316a2.jpg)  
**图 6: 轻量级神经网络。** 该图描绘了在 4.3 节中解释的 LinnOS 3 层神经网络。

**输入特征。** 为了推断每个 I/O 的速度，我们的模型需要三个输入：(a) 一个传入 I/O 到达时挂起的 I/O 数量（以 4KB 页为单位，包括传入的 I/O），(b) R 个最近完成的 I/O 的延迟，其中我们设置 R 为 4，以及 (c) 这 R 个已完成 I/O 到达时挂起的 I/O 数量。我们现在来论证这些必要输入。

决定第一个特征是直接的——一个 I/O 延迟通常与当前挂起的 I/O 数量相关。我们这里使用的单位是 4KB 挂起页面的数量，原因在于 SSD 内部条带化的最低粒度通常是页级别，而主要的争用发生在通道和芯片级别。

对于磁盘来说，第一个特征可能足以推断单轴性能，但对于 SSD，还需要另外两个特征。本质上，为了推测 SSD 当前内部是否繁忙，我们需要记录一小段历史信息，即最后四个 I/O 的延迟，以及这些 I/O 到达时存在的挂起 I/O 数量。简而言之，如果最近的 I/O 在没有很多挂起 I/O 的情况下经历了长延迟，那么模型可以学习到可能存在由于设备级活动（如 GC、内部刷新或磨损均衡）导致的内部争用。在这种情况下，模型将建议撤销传入的 I/O，直到挂起的 I/O 数量大幅下降，以便设备即使在内部活动繁忙的情况下也能提供快速响应。一旦设备恢复服务 I/O，模型可以从返回的延迟值中判断设备级争用是否结束。

我们上面的特征看起来很简单，因为我们经过多次设计迭代后移除了不必要的特征。例如，我们惊讶地发现，一些看起来很重要的特征，如块偏移、读/写标志或长期的写历史，对提高准确性并没有显著作用。我们做出几个推测。首先，关于读/写标志，尽管 NAND 级的读/写延迟不同，但几乎所有中高端 SSD 都采用写缓冲。因此，读后写的问题不再能观察到。更可能观察到的是读后缓冲区刷新延迟，这可以从我们的输入特征中学习到。其次，关于块偏移，因为我们针对的是生产工作负载，并且 SSD 通常会将传入的 I/O 均匀地条带化到所有通道和芯片（或进行一些有界分区），工作负载很可能被均匀分散，因此块偏移对于学习来说并不重要。换句话说，一批具有同时命中单个芯片的块偏移的传入 I/O 的情况在现场很少发生。第三，关于写历史，内部活动如 GC 和缓冲区刷新通常以短时间的突发形式发生，因此仅通过观察最后四个 I/O 的速度就可以感知到。这些是令人惊讶但幸运的发现，因为仅使用一小组特征将减少模型的开销。

**输入格式。** 下一个挑战是选择正确的输入格式来馈送给神经元。首先，对于 R 值，如果准确性是唯一重要的指标，我们应该记录更多已完成的 I/O（R 越大越好），但这会延长推理时间，因为神经元的数量会增加。我们发现 R=4 足以平衡性能和准确性。

在另一个简化中，我们将挂起的 I/O 数量格式化为三位十进制数。例如，15 个挂起 I/O 的格式是三个整数 {0, 1, 5}。三位数足够了，因为设备队列长度超过 1000 的情况很少听说。类似地，对于最近完成 I/O 的延迟，我们将微秒级的延迟值分解为四位数。例如，一个最近完成的 I/O 延迟为 240μs，将被格式化为四个特征 {0, 2, 4, 0}。大于 9,999μs 的延迟将被限制为 {9, 9, 9, 9}。总共，我们的模型需要 31 个输入特征，每个都是一位十进制数。

将原始整数重新格式化为十进制数字是一种有效的权衡。如果我们使用位并将每个位提供给每个神经元，会有太多的神经元，这会增加模型大小并损害推理时间。在另一个极端，如果每个神经元接受一个原始整数值，神经元需要在很宽的输入范围内学习，这使得学习/训练更加困难（例如，延迟值可以从 1μs 到超过 9,999μs）。通过使用十进制数字，我们使神经元的学习范围限制在 0 到 9 的小范围内。

**网络。** 最终的模型是一个只有三层（“轻量级”）的全连接神经网络，包括一个输入/预处理层、一个隐藏层和一个输出层，如图 6 所示。所有神经元都是常规的线性神经元 ($y = wx + b$)。

输入层由上述 31 个特征提供。来自块层的原始信息被转换为特征格式，训练时是离线方式，实时推理时是在线方式。对于后者，通过一些编程优化，我们可以实现 O(1) 的预处理开销。接下来，隐藏层由 256 个常规神经元组成。该层使用 ReLU 激活函数，因其计算成本低且能支持非线性建模。更多的神经元会导致更长的推理时间，而更少的神经元则准确性较低。最后，输出层有两个带有线性激活函数的神经元。我们使用 argmax 运算符将输出转换为二元决策（例如，{0.4, 0.6} 转换为 {0, 1}）。总的来说，这种设计使网络轻量级且易于集成到操作系统中，同时平衡了推理准确性和性能。

**先前的设计迭代。** 这里我们简要描述我们如何达到当前的设计。我们最初使用 I/O 偏移的二进制格式（32 位）作为输入特征，因为设备的 FTL 映射基本上使用 I/O 偏移来决定 I/O 的去向，这定义了资源争用。这种设置允许学习模型实现更高的准确性（对于某些踪迹高达 99%），但它模型庞大，推理开销高，不适用于实时使用。我们进一步修剪了庞大的模型，但无法在通用性和推理开销之间找到合理的权衡。因此，我们从细粒度的特征后退一步，转向更聚合的特征，最终达到了当前的设计。

# 4.4 提高准确性

为了进一步提高模型准确性，我们通过偏置训练进行错误提交减少，通过重新追踪/重新训练进行模型重新校准，以及通过高百分位对冲来掩盖不准确性。

**减少错误提交。** 准确的推断意味着 LinnOS 提交那些将是快速的 I/O（真阴性），并撤销那些将是慢速的 I/O（真阳性）。反之，不准确的情况可以分为 (a)“错误提交”（假阴性），即模型认为请求将被快速处理，导致 LinnOS 将请求提交给设备，但请求花费的时间超过了快慢阈值，或 (b)“错误撤销”（假阳性），即 I/O 被撤销，但实际上它可以被设备快速处理。

利用我们对现场典型延迟分布的系统直觉（第 4.2 节），我们发现减少错误提交要重要得多，而错误撤销则更容易容忍。当集群的存储设备表现出相似的尾部行为（高 alpha 帕累托分布）时，对等设备同时繁忙的概率相对较小。例如，有三个副本和 P% 的繁忙率，所有副本大约在同一时间繁忙的概率是 $(P/100)^3$（例如，5% 繁忙率时为 0.000125）。另一个因素是，随着网络速度的加快，故障转移成本可以低至 PCIe 或光纤通道上的闪存阵列的 $1-6\mu s$，或以太网上的 $5-40\mu s$（加上一些可忽略的软件开销）。

总而言之，错误推断的惩罚对于错误撤销来说很小，但对于错误提交来说很高。在后一种情况下，I/O 将被“卡”在设备中无法被撤销。这促使我们使用偏置训练来减少错误提交，方法是允许更多的错误撤销。我们通过自定义分类合页损失函数（categorical hinge loss function），为其增加一个乘数，对错误提交施加更多的惩罚权重，这使得训练出的模型更倾向于进行错误撤销。

**重新校准。** 另一个不准确的来源发生在根据训练数据计算出的拐点不能代表“测试”数据（实时推理期间的工作负载）的相同阈值时。这可能在工作负载发生显著变化，导致集群中节点的延迟分布发生偏移时发生。幸运的是，我们对生产踪迹的评估显示，延迟分布在不同小时之间没有广泛的偏移（§5.2）。然而，为了预见这种情况，可以每隔几小时定期进行重新追踪和拐点分析的重新计算。如果在新的工作负载-设备对中，拐点偏移了五个百分位数，LinnApp 将使用新收集的踪迹重新训练模型，并将新的训练权重重新上传到设备。在生产工作负载的最繁忙时段运行 blktrace 仅产生 300MB 的数据（85 KB/s 的踪迹写入），并使 CPU 开销增加 0.5%（仅追踪相关参数）。

**掩盖微小的不准确性。** 我们上述的方法成功将准确率提高到 98%。就像其他神经网络一样，实现 100% 的准确率从根本上是困难的，并且通常意味着缺乏通用性。在微小的不准确性范围内，由于错误提交导致的长延迟尾部仍然需要规避。这就是我们将学习和对冲（hedging） 结合起来的地方。当错误提交率（第 5.4 节）很显著时（例如，>5%），我们使用该比率作为对冲百分位值的指标。例如，如果 6% 的推断产生错误提交，那么将应用 p94 对冲。当错误提交率较低时，我们向上取整到传统的 p95 对冲。尽管这种设计有时会发出额外的 I/O，但我们表明它可以进一步提高性能（§5.3）。

# 4.5 提高推理时间

深度神经网络（DNN）研究的很大一部分主要集中在如何构建更大的网络以实现尽可能高的准确性。严格的延迟通常不是一个约束。然而，将神经网络放入存储层提出了一个独特的挑战。我们的目标是达到大约 $5\mu s$ 的推理时间（如 §3.3 所讨论），尽管 3 层设计是实现这一目标的基础，我们还进行了进一步的优化。

**量化。** 首先，神经元权重默认是浮点数以提高准确性，但对于我们的目的来说这是矫枉过正。定义争用的一些主要存储功能，如使用整数上的模运算进行条带化和分区，并不需要超高精度。此外，浮点计算成本高昂且在操作系统内部难以管理。因此，我们采用 DNN 量化，保持三位小数的精度；训练好的浮点权重被转换为具有三位小数精度的整数。DNN 量化是一种流行的技术，用于减少 DNN 在移动平台和物联网设备上的空间、功耗和计算成本，尽管会损失一些准确性。在我们的案例中，量化带来的准确性损失小于 0.1%。

**协处理器。** 其次，使用额外的加速器如 GPU 和 TPU 在未来可能是可行的，但目前它们更倾向于优化吞吐量，并且不容易与主机内核代码交互。如果我们将推理移到 GPU，跨设备通信会增加更多开销。此外，技术趋势表明，在不久的将来，通过更先进的硬件，推理延迟可以预见到 100-200 倍的改进。这可能使 LinnOS 在未来更快，特别是随着存储设备也变得越来越快。然而，在这种技术到来之前，我们表明 LinnOS 可以机会性地使用协处理器（如果可用）来将平均推理时间从 6μs 减少到 4μs，通过使用一个额外的 CPU 核进行 2 线程优化的矩阵乘法。

# 4.6 优势总结

通过所有这些技术，LinnOS 在各个维度上都带来了优势，我们将在评估中展示这些优势。

- **性能可预测性。** 最重要的优势是 LinnOS 帮助存储应用程序在闪存阵列上实现可预测的性能，优于其他流行方法。
- **自动化。** LinnOS 通过学习数百万个 I/O 来推断 I/O 操作延迟，并为不同的工作负载和设备自动训练和生成神经元权重。存储开发人员不必手动调整和配置启发式方法。
- **通用性。** 为了实现可预测性，LinnOS 不需要设备级的修改，也不需要对文件系统或应用程序进行大规模的重新设计。存储应用程序只需标记对延迟敏感的 I/O。故障转移/重试逻辑在许多具有数据副本的存储应用程序中已经是标准配置。
- **及时性。** 通过快速推理，应用程序可以在收到慢速错误码后立即进行故障转移，无需等待超时。
- **效率。** 通过自动撤销，LinnOS 消除了对冲中遭受的重复 I/O。一些生产系统出于同样的原因不使用对冲，而是使用更有效的方法，如“捆绑请求”（tied requests），即发送克隆请求，但当其中一个被服务后，重复的请求被取消。类似于这种“克隆后取消”的方法，我们的“撤销后故障转移”也避免了重复。此外，虽然一些“捆绑请求”的实现给应用层带来了负担，但 LinnOS 在内核内部支持 I/O 撤销。
- **简单性。** 我们不要求应用程序提供 SLO（服务水平目标）值，如截止时间。今天的 I/O 系统调用不接受 SLO 信息，可以说是因为设置合适的 SLO 并不容易。LinnOS 通过自动调整的快/慢二元分类简化了这一点。

# 4.7 实现复杂性

LinnOS 扩展了 Linux v5.4.8，在块层中增加了 2170 行代码（LOC），主要用于神经网络模型（用 C 语言编写）和简单的撤销机制。一个神经网络模型（总共 8706 个权重和偏置）所需的内存空间为 68 KB 的内核内存。LinnApp 用 3820 行代码编写，包括数据收集、分析、标记、训练（使用 TensorFlow）和量化。我们将源代码公开（A 节）。

# 5 评估

在本节中，我们首先描述我们的评估设置（第 5.1 节），然后展示回答以下重要问题的结果：

- **稳定性（第 5.2 节）：** 我们的拐点算法对于生产工作负载是否足够稳定？
- **延迟可预测性（第 5.3 节）：** 与其他方法相比，LinnOS 是否成功地提供了更可预测的延迟？
- **模型准确性（第 5.4 节）：** LinnOS 神经网络在推断每次 I/O 速度方面的准确性如何？
- **权衡（第 5.5 节）：** LinnOS 在性能和准确性方面存在哪些权衡？
- **其他（第 5.6 节）：** LinnOS 在其他公开踪迹上表现如何？LinnOS 能否支持全栈存储应用程序？CPU 开销是多少？

# 5.1 设置

我们介绍了评估所用的工作负载、设备、实验以及我们进行比较的方法。

**工作负载。** 我们的最终目标是评估 LinnOS 是否能帮助真实的生产场景。我们使用了来自微软 Azure (AZ)、Bing Index (BingI/BI)、Bing Select (BingS/BS) 和 Cosmos (CO) 服务器的 SSD 级踪迹。每种服务器类型包含六个设备的 I/O 踪迹。平均每个踪迹包含 36 小时的 I/O 操作。对于训练数据，我们从四种服务器类型中各挑选了三个最繁忙的设备踪迹，然后挑选了最繁忙的一小时（相同的三个小时）；我们限制为三个是由于我们拥有的（昂贵的）企业级 SSD 的数量（下文详述）。对于专用于实时实验的“测试数据”，我们从其他繁忙时段中随机选择一个时间片，因此训练和测试数据不重叠。总的来说，训练和测试数据并未占据所有可用的踪迹。

**SSD 设备。** 为了进行性能评估，我们展示了 LinnOS 在帮助闪存阵列提供可预测延迟方面的效果。我们准备了两个闪存阵列，分别配置了消费级（“C”）和企业级（“E”）。前者连接了三个同构的 SM951 消费级 SSD 阵列，后者则由三个异构的企业级 SSD 组成：Intel P4600、Samsung PM1725a 和 WD Ultrastar DC SN200。我们假设每个数据块在三个设备上有三个副本，这是面向消费者的存储服务器的典型设置。对于这两种配置，机器都配备了 2.6GHz 18 核（36 线程）的 Intel i9-7980XE CPU 和 128GB DRAM。除非另有说明，我们不使用加速器（§4.5）。故障转移被撤销的 I/O 的开销为 15μs。对于准确性评估，除了这四种闪存型号，我们还使用了 Intel SSDSC1BG40、Intel SSDSC2BX01、Intel P3700、Intel P4510、Intel S3700 和 Samsung 960 EVO，总共 10 种型号。在本次评估之前，所有设备都已在多种工作负载下使用了数月，达到了设备的满负荷状态，从而模拟了现场使用的设备。

**实验。** 对于性能评估，实验是通过一个在闪存阵列上执行踪迹的存储应用程序进行的，其中所有设备都服务于读/写工作负载。例如，在一个实验中，应用程序同时在消费级闪存阵列的三个独立 SM951 设备上执行三个不同的 Azure 踪迹，并记录已完成读取 I/O 的延迟。该应用程序具有故障转移能力，可以在其他设备上完成被撤销的 I/O（如前图 2 所示）。所有读取 I/O 都被标记为延迟关键。我们也知道这些踪迹是在微软的中端设备上收集的（2016 年）。因此，对于我们的高端闪存阵列配置，我们必须通过重新调整踪迹使其更密集来模拟更重的工作负载。我们的方法是，对于每个重新调整的踪迹，其在高端设备上运行后产生的基线延迟分布应与原始踪迹中的延迟分布相似。

**表 1: 重定标踪迹的 I/O 特性 (§5.1)**。上半部分（前四行）用于消费级闪存阵列，下半部分用于企业级闪存阵列。每个 max-IOPS 值是在 10 秒窗口内测量的。

| 测试踪迹 | 平均 IOPS | 最大 IOPS | 读:写比例 | 平均 I/O 大小 读/写 | 最大 I/O 大小 读/写 |
|---|---|---|---|---|---|
| AZ/C | 745 | 4.9K | 27:73 | 24K/18K | 64K/64K |
| BI/C | 361 | 1.8K | 17:83 | 57K/30K | 64K/1M |
| BS/C | 114 | 1.1K | 22:78 | 163K/73K | 2M/9M |
| CO/C | 113 | 623 | 32:68 | 479K/121K | 6M/32M |
| AZ/E | 13K | 31K | 25:75 | 25K/17K | 64K/64K |
| BI/E | 2.4K | 9.2K | 23:17 | 55K/30K | 512K/1M |
| BS/E | 1.3K | 4.3K | 27:73 | 196K/73K | 2M/9M |
| CO/E | 2.5K | 7.2K | 22:78 | 430K/107K | 7M/32M |

在我们的驱动器上运行这些重定标的踪迹通常显示出较低的空闲时间（<5% 的所有 I/O），即 SSD 在几毫秒内没有待处理的 I/O，以及明显的突发性（5-30%），即 I/O 需要在操作系统中等待，因为 SSD 队列已满。我们相信这能准确地模拟真实部署中看到的空闲和尾部行为。此外，跨设备的负载突发高度相关，这在某些情况下可能导致无法通过故障转移处理的长尾行为。然而，在实际运行中我们发现，由于设备级的复杂性，设备的内部繁忙程度不一定相关，LinnOS 通过规避底层设备的特性显示出了巨大的改进（§5.3）。所有实验重复三次，未观察到显著差异。

**比较方法。** 我们进行了广泛的评估，比较了八种方法：基线、克隆、恒定百分位对冲（例如，在 p95 延迟）、拐点对冲（使用我们的算法）、简单启发式、高级启发式、LinnOS（单独使用）以及 LinnOS 与高百分位对冲。将 LinnOS 与白盒方法 进行比较超出了本文的范围，因为 LinnOS 针对的是黑盒设备，而我们无法访问可编程设备阵列。

# 5.2 拐点（IP）稳定性

本文的贡献之一是找到了半最优的快/慢拐点（IP），它在及时性和开销之间取得了平衡（图 5，第 4.2 节）。表 2 显示了我们的算法为每个工作负载-设备对计算的 IP 值。每个单元格中的三个数字代表三个不同的踪迹（来自同一服务器类型），每个踪迹在闪存阵列中的一个 SSD 上运行。如图所示，IP 值范围从 p72 到 p98，这突显了为什么恒定的超时值不是最优的，并且会损害性能。这些 IP 值将用于快/慢标记和训练，然后为每个设备生成一组独特的权重。

![](https://cdn-mineru.openxlab.org.cn/result/2025-07-07/6c1e16d3-ec8f-454c-b865-5fc71eb2fec6/1ea62be594c5aafd0f96a1a11e5e132ab704f4c122038b7001b9d5caa355b787.jpg)
**图 7: IP 稳定性。**

**表 2: 拐点（IP）设置。** 该表如第 5.2 节所述，显示了我们的算法在第 4.2.1 节中为每个工作负载-设备对计算的 IP 值。

| | 消费级 | 企业级 |
|---|---|---|
| Azure | p73.3, p77.0, p91.4 | p91.0, p93.2, p97.8 |
| BingIndex | p80.0, p94.5, p98.5 | p80.1, p83.3, p97.0 |
| BingSelect | p72.0, p76.9, p87.2 | p75.3, p83.7, p86.8 |
| Cosmos | p73.4, p82.5, p84.1 | p83.2, p84.8, p95.1 |

我们选择了一个繁忙时段（T=1小时窗口）来收集训练数据并计算该时间片的 IP 值。图 7 通过绘制接下来 20 小时（x 轴）内不同 T 窗口值的最大 IP 偏差（百分位数，y 轴）来显示我们方法的稳定性。例如，如果所选小时表现出 p85 IP，但随后的一个小时表现出 p75 或 p95 IP，那么偏差就是 10 个百分位数（y=10）。该图显示，如果 T=1 小时窗口，偏差在接下来的 15 小时内被限制在五个百分位数以内，表明频繁的重新训练是不必要的。如果 T 更短（例如，15 分钟窗口），偏差更明显（需要频繁的重新训练，这通常在 CPU 上 15-20 分钟内收敛，因为 LinnOS 的模型很轻量）。如果 T 更大（2 小时窗口），增益不显著。为了一般性，该图是我们算法在所有数据集（每个踪迹 36 小时，24 个踪迹，四种服务器类型）上模拟的结果。

延迟重新训练的代价取决于偏差。让我们举个例子，一个模型为 p95（5ms）训练，但后来工作负载发生偏离，使得真实的 IP 位于 p90（10ms），因为工作负载变得更加写密集。在这种情况下，LinnOS（仍然使用 5ms）将过度撤销许多本可以在 10ms 内完成的 IO（更多的错误撤销）。如果故障转移开销可以忽略不计，这不会造成太大伤害。另一种情况是工作负载偏离，使得 IP 上升到 p99（3ms）。在这里，LinnOS 会过度接受（更多的错误提交），因为 3-5ms 的延迟被错误地认为是“快”，但实际上可以更快。这就是 LinnOS 在不重新训练的情况下会受损的地方。

# 5.3 延迟可预测性

我们现在评估 LinnOS 在实现极高延迟可预测性方面的成功。图 8 显示了在两种闪存阵列（消费级和企业级）上八种方法的平均 I/O 延迟（用户感知）。更详细地，图 9 显示了特定百分位数（x 轴上从 p80 到 p99.99）的延迟。下面我们剖析每种方法的优缺点。我们从基线开始，然后跳到“LinnOS+HL”（最佳结果），接着是其他方法。

**基线。** 图 9 中的基线证实了闪存存储的不可预测性，其延迟在 p95 到 p99.99 之间几乎呈指数级飙升，与我们的最佳情况相比，平均延迟增加了 1.3-6.5 倍（图 8）。显然，具有数据冗余的闪存阵列应采用削减尾部延迟的方法以实现更高的可预测性。

**LinnOS+HL。** 该标签代表 LinnOS 方法与高百分位对冲相结合，用于掩盖神经网络中固有的、难以消除的微小不准确性（第 4.4 节）。也就是说，为了补偿导致错误提交的不准确性，我们的应用程序在 pX 延迟时间过去后发送一个重复的 I/O，其中 X 是 95 和 (1 - 错误提交率) × 100 中较小的值。我们使用训练过程中的错误提交率（图 10，第 5.4 节）。

**[关键结果] →** 图 8 中的平均延迟显示，LinnOS+HL 在不同工作负载和平台上始终优于所有其他方法。平均而言，与 p95 对冲（Hedge95）相比，LinnOS+HL 将延迟降低了 9.6% - 79.6%；与使用我们的 IP 算法的对冲（HedgeIP）相比，降低了 14.2% - 49.5%；与高级启发式（HearAdv）相比，降低了 10.7% - 71.2%。这些加速是稳定延迟的产物；在图 9 中，LinnOS+HL 的线即使在极高的百分位数 p99 到 p99.99 也表现出稳定的延迟。这些结果得出了一个积极的结论：LinnOS 的缺点（15μs 的故障转移开销，包括每次 I/O 6μs 的推理成本和不准确性）被其在提供可预测延迟方面的有效性所抵消。

**LinnOS (Raw)。** 这里我们展示了即使没有对冲（即，撤销+故障转移而没有 I/O 复制），LinnOS 的效率。图 8 中的 LinnOS-Raw 条形图显示，LinnOS 本身就足够有效，仅比 LinnOS+HL 差 1.3% - 45.7%，而与 p95 对冲相比，LinnOS-Raw 将延迟降低了 0.3% - 62.3%，与高级启发式相比，降低了 3.0% - 60.7%。图 9 详细说明了为什么添加对冲是有用的。在高百分位数，即 p99 以上，LinnOS-Raw 开始表现出高延迟（由于错误提交）。从对冲的“小尾部”行为（例如，Hedge95 线）中学习，我们在 LinnOS+HL 中结合了两者的优点。

![](https://cdn-mineru.openxlab.org.cn/result/2025-07-07/6c1e16d3-ec8f-454c-b865-5fc71eb2fec6/8de3196e583811f7135578b845c1a22ec4002e15d13a508ccd7cf9a97f1e3bfc.jpg)
**图 8: 平均延迟。** 图表显示 LinnOS 始终优于所有其他方法，如第 5.3 节所述。顶部和底部的图表分别代表在消费级和企业级阵列上的实验。

![](https://cdn-mineru.openxlab.org.cn/result/2025-07-07/6c1e16d3-ec8f-454c-b865-5fc71eb2fec6/13d2c4fc87a9bcf39fbf1076a79dfd7bd2e7a4592a4215213022f65780fbb469.jpg)  
**图 9: 百分位延迟。** 如第 5.3 节所述，图表显示 LinnOS+HL 在所有百分位（x 轴）上，即使在 p99.99，也提供了最可预测的延迟（y 轴）。在图 (a) 中，“AZ/C”表示在消费级阵列上运行的 Azure。

**在 p95 进行对冲。** 在 p95 延迟超时后发送一个重复的 I/O 是业界常用的一种方法。图 9 显示，总的来说，这种方法在削减延迟尾部方面是有效的，但通常比 LinnOS+HL 产生更高的延迟。这是因为 Hedge95 需要等待超时发生后才能发送重复的 I/O，而 LinnOS 返回一个及时的撤销，允许应用程序快速进行故障转移。因此，Hedge95 平均比 LinnOS+HL 甚至 LinnOS-Raw 慢（图 8）。

**在 IP 进行对冲。** 表 2 中显示的许多 IP 值都低于 p95，这引发了一个问题：在 IP 进行对冲是否会比在 p95 好。图 8 中的平均值显示了混合的结果。在消费级设备上，HedgeIP 在重负载 BingS 和 Cosmos 上比 Hedge95 提高了 2 倍，但在轻负载 Azure 和 BingI 上则损失了高达 15%。类似地，在企业级设备上，HedgeIP 在 BingS 上获胜，而在其他设备上略有损失。经过进一步调查，我们发现，例如，在消费级设备上，Azure 和 BingI 的延迟通常很快（分别 < 2ms 和 10ms，如图 9a-b 的 y 轴所示），因此对来自重复 I/O 的额外负载很敏感；在我们的实验中，HedgeIP 发送的重复 I/O 比 Hedge95 多。然而，我们的实验表明，对于大多数工作负载，HedgeIP 比 Hedge95 更有效，因此具有对冲功能的系统可以采用我们的 IP 算法。

**简单启发式。** 我们编写的第一个启发式方法“HeurSim”基于一种流行的旋转磁盘启发式：如果设备队列长度（未完成的 I/O 数量）大于一个阈值，则传入的 I/O 应该在别处重试。对于阈值，我们使用了与 HedgeIP 类似的方法，但我们使用的是 IP 队列长度而不是 IP 延迟值。也就是说，我们首先在追踪期间分析队列长度分布，然后选择 IP 百分位处的队列长度作为撤销的阈值。图 8 显示，HeurSim 仅比基线有小幅改进，远未达到最佳情况。简而言之，它不够智能，无法推断设备内部的中断。

**高级启发式。** 我们将 HeurSim 扩展为一个更“高级的启发式”，HeurAdv。为了比较的公平性，我们重用了构建 LinnOS 时的相同直觉，并将其应用于 HeurAdv。HeurAdv 执行的额外任务是扫描最后 N 个完成的 I/O（N=4，与 LinnOS 中相同），如果这段历史显示有一个慢 I/O（“慢”的定义见 §4.2）但队列长度很低（小于中位数），它将把驱动器标记为“内部繁忙”。在这种状态下，除非队列长度下降到很低的值（小于下四分位数队列长度），否则不会接纳传入的 I/O。状态不会从“繁忙”变为“正常”，直到它看到最近的 I/O 变得“快”（“快”的定义见 §4.2）。

**[第二个关键结果] →** 图 8 显示，HeurAdv 在大多数情况下优于 HeurSim，但仍然输给其他方法。我们想指出，我们花了数周时间将这个启发式调整到我们能达到的“最佳”结果。继续扩展和调整启发式是可能的。然而，将出现的主要困难是参数的设计空间很大（正常/繁忙状态、中位数和下四分位数队列长度等），这些参数必须为不同的工作负载和设备进行最优和手动的配置。这正是我们展示机器学习有所帮助的地方。使用轻量级神经网络使我们能够专注于决定哪些特征重要，同时让模型学习和逆向工程 SSD 的行为。在我们的案例中，LinnOS 神经网络为不同的设备和工作负载自动训练了所有 8706 个权重。

![](https://cdn-mineru.openxlab.org.cn/result/2025-07-07/6c1e16d3-ec8f-454c-b865-5fc71eb2fec6/2f101a35a306f1cadba593c59d00bb976a5eb378f324c718274e116f12bed1d1.jpg)
**图 10: 低不准确率。** 该图显示了错误提交和错误撤销的百分比。请注意，只有错误提交真正重要（见第 5.4 节）。此外，“P”代表我们可以从公共云访问的其他设备型号。为了图表的可读性，这里我们只显示了一个设备型号的“P”结果，而观察结果在其余设备上仍然成立。总的来说，准确性评估涵盖了 10 种设备型号（IC + 3E + 6P）。

**克隆。** 这种方法本质上是 p00 对冲，即为每个 I/O 一开始就发送一个重复的 I/O。尽管 SSD 速度快且具有内部并行性，图 8 显示，由于 2 倍的负载，Clone 大部分情况下比基线差。

# 5.4 （低）不准确率

我们现在通过计算错误提交和错误撤销的数量来衡量 LinnOS 的不准确性（第 4.4 节）。实时实验只能测量前者而不能测量后者。这是因为被撤销的 I/O 从未提交给设备，因此我们永远不知道撤销是否准确。因此，对于本次评估，我们使用 TensorFlow 以离线方式衡量不准确性，就像训练阶段一样。但是，请注意，训练和测试数据都是通过在真实闪存阵列上运行工作负载收集的（即，不是模拟数据）。和之前一样，我们使用 1 小时的数据集进行训练，然后选择三个不同的 1 小时数据集进行测试准确性，并测量平均不准确性。

图 10 显示了我们使用偏置训练前后的不准确性。回顾第 4.1 节，错误提交比错误撤销更危险。在没有偏置的情况下，上图显示错误提交率（红色条）很高，在 1.3% 到 10.8% 之间。通过偏置训练，如下图所示，我们成功地将错误提交率降低到 0.7-5.7%，方法是将不准确性转移到错误撤销上，这在第 4.4 节中解释过，是更容易容忍的。例如，让我们假设一个较差的场景，即 p80 拐点（即 20% 的慢速），这意味着所有三个副本都慢的概率是 0.008 ($((^{20} / 100)^{3})$)。因此，尽管我们在图 10b 中将错误撤销率提高到了 2.8-9.7%，但只有 0.008 的这些错误撤销在概率上会导致慢速 I/O。最后，如前所述，为了掩盖危险的低不准确性（0.7-5.7% 的错误提交），将 LinnOS 与高百分位对冲 (LinnOS+HL) 相结合，得到了一个强大的结果。

# 5.5 权衡

表 3 显示了推理开销和准确性之间的一些可能权衡（模型 A-E，准确性涉及错误提交和错误撤销）。一方面，如果倾向于更低的开销并且可以接受一些准确性损失，那么一个选择是削减输入特征和模型。例如，在模型 B 中，R=3（即，包含更少的历史 I/O 而不是 R=4）可以将输入特征的数量从 31 个减少到 24 个，并降低推理开销 -1μs，但这会带来一些准确性损失 -(1-4%)，因为输入更少。或者，如果更倾向于更低的开销，那么在模型 A 中，我们可以进一步削减输入特征（R=2，17 个特征）并使用更薄的隐藏层（从 256 个神经元到 128 个），从而获得更低的推理时间 -4μs，但带来更大的准确性损失 -(3-12%)。

| 模型 | A | B | C | D | E |
|---|---|---|---|---|---|
| 准确率 (%) | -(3-12) | +(1-4) | +(1-2) | +(4-5) | +(8-12) |
| 性能 (μs) | -4 | -1 | +40 | +94 | +1670 |
**表 3: 权衡平衡。** 该表在第 5.5 节中解释。所有准确性和性能的 +/- 值都是与我们在 §4 中描述的最终神经网络模型相比的。

如果需要更高的准确性，那么我们可以引入更多的特征和更重的模型。例如，在模型 C 中，通过向模型添加一个隐藏层，我们可以获得 +(1-2%) 的更高准确性，而推理开销则上升了 +40μs。更进一步，我们可以涉及更多的特征（高达 R=10 和 73 个特征）和更多的隐藏层（三层，256-512-256 个神经元）来将准确性增益推高 +(4-5%)，但开销增加到 +94μs。极端的模型 E 在输入特征中包含了块偏移（总共 2048 个特征）并应用了一个有五个隐藏层（每层 512 个神经元）的模型。对于某些踪迹，这个模型将准确性提高了 +(8-12)%，但其推理开销 +1670μs 对于实时推理来说极高。

# 5.6 其他评估

# 5.6.1 附加性能评估

**其他可能的手动调整启发式。** 为了了解启发式方法最终能达到多高的性能，我们从踪迹中挑选了几个 10 分钟的片段，并手动调整 HeurSim 和 HeurAdv 的可调参数，使用各种阈值，直到达到最佳结果。简而言之，我们从通用的 HeurSim 和 HeurAdv 开始，用切片后的踪迹评估它们，跟踪未被撤销的高延迟 I/O，更新阈值以捕获这些 I/O 而不引起太多错误撤销（例如，>15%），重新评估并重复整个过程，直到观察到近似最优。这种方法确实能够让启发式方法进一步发挥作用。例如，我们看到一些情况下，调整后的启发式可以在 p95 上比 LinnOS-Raw 表现好高达 20%。然而，这个调整过程繁琐且在实际运行中不切实际，因为重复的手动调整太慢，无法跟上输入工作负载的波动。

**LinnOS+H99。** 我们也尝试了 LinnOS+H99，它采用 p99 对冲，只产生 1% 的额外 I/O。图 11a 显示了它与 LinnOS+HL 的一个比较。通常，LinnOS+H99 由于等待时间更长而遇到更大的尾部区域，但在较低百分位数处由于额外 I/O 较少而响应更快。因此，有时 LinnOS+HL 的平均延迟可能比 LinnOS+H99 稍差（高达 3%）（图 11b）。然而，在我们绝大多数的基准测试中，LinnOS+HL 的平均延迟比 LinnOS+H99 好 1.7-39.2%。

![](https://cdn-mineru.openxlab.org.cn/result/2025-07-07/6c1e16d3-ec8f-454c-b865-5fc71eb2fec6/a2f5977f3aedc10d16991b137d4f2c1b5dce0d49e3be6f937ac7efd5aebe0535.jpg)
**图 11: LinnOS+H99。**

# 5.6.2 在公开踪迹上的表现

除了我们对微软踪迹的评估，图 12 显示了在我们的消费级闪存阵列上运行 SNIA 网站上发布的最新 SSD 踪迹 的快速评估。结果证实，LinnOS 也表现出低不准确性（图 12a）和显著的延迟改善（图 12b）。

![](https://cdn-mineru.openxlab.org.cn/result/2025-07-07/6c1e16d3-ec8f-454c-b865-5fc71eb2fec6/44f4c14f4ca9b36712311e198ecc52d422a214495d041804de35920f9c8472b0.jpg)
**图 12: 在公开踪迹上的表现。** 如 §5.6.2 中所述。

# 5.6.3 MongoDB 在不同文件系统上的表现

为了观察数据应用如何从 LinnOS 中受益，我们在我们的三个企业级驱动器上建立了一个本地 MongoDB 副本集，并采用同构的文件系统设置。对于每种文件系统类型，MongoDB 接收 120K 个随机读取请求，并且所有驱动器在为 MongoDB 请求（作为延迟关键 I/O）服务时都运行微软踪迹作为背景噪音。在这里，我们关注高百分位延迟（例如，p99 延迟），因为平均延迟在很大程度上受到文件系统缓冲的影响，而尾部延迟则反映了设备的原始性能。

图 13 显示，通过 LinnOS，MongoDB 实现了更可预测的性能。例如，当所有底层设备都使用 f2fs 格式化时，LinnOS 将 p99 延迟降低了 76.7%。此外，LinnOS 只需要对 MongoDB 和文件系统进行微小的更改：增加 50 行代码。例如，文件系统应该直接将 LinnOS 的错误码返回给应用程序，而不是进行不必要的自检，而 MongoDB 需要稍作修改以重用其内置的故障转移逻辑。

![](https://cdn-mineru.openxlab.org.cn/result/2025-07-07/6c1e16d3-ec8f-454c-b865-5fc71eb2fec6/f3b4eeb0b15340df4d7a122bea3bebe3232a9919406120509ac3595fe6169e85.jpg)
**图 13: MongoDB 在不同文件系统上的表现。** 该图显示 LinnOS 可以轻松帮助数据应用程序实现更可预测的延迟（§5.6.3）。

# 5.6.4 计算开销/优化

**CPU 开销。** 一个合理的担忧是，如果整个操作系统有许多神经网络，那么它将是 CPU 密集型的。在所有基准测试和 SSD 中，与轻量级神经网络配对，每个设备仅占用 0.3-0.7% 的主机 CPU 资源，使得 LinnOS 对于大规模部署是可行的。

**使用协处理器加速。** 如第 4.5 节所述，可以利用额外的处理器来加速推理。通过利用一个额外的 CPU 核，LinnOS 可以将推理开销降低 36%（至 4μs），而每个设备的最大 CPU 使用率增加到 1.4%。

# 6 结论与讨论

我们介绍了 LinnOS，据我们所知，这是第一个能够推断每次 I/O 到闪存存储速度的操作系统。我们已经展示了在操作系统中使用轻量级神经网络进行频繁、细粒度、黑盒实时推理的可行性。LinnOS 的性能优于许多其他方法，并成功地在不可预测的闪存存储上带来了可预测性。我们还认为，LinnOS 的成功引出了激动人心的讨论和问题，可以激发未来的工作：

**关于性能。** 尽管 LinnOS 的推理开销（4-6μs）与当前 SSD 的访问延迟（例如 80μs）相比不那么明显，但随着 SSD 向 10μs 延迟范围迈进，这可能成为一个问题。此外，随着 IOPS 的增长，计算资源的消耗可能会大幅增加。如何进一步降低推理成本（例如，到 1μs）以支持更快的设备和更高的吞吐量？先进的加速器能帮助加速操作系统内核操作吗？近存储/数据处理能有所帮助吗？当结果高度确定时（例如，队列长度非常低），我们能否跳过推理？我们能否为流行的预测缓存近似结果？

**关于掩盖机器学习的不准确性。** 由于机器学习（例如 LinnOS）永远无法达到 100% 的准确性，“ML-for-system”解决方案应该如何掩盖机器学习未能捕捉到的情况，同时仍然从其通用性中受益？将学习和启发式结合（例如，像 LinnOS+HL 中那样）是否是一个强大的选择，可以利用两者的优势？

**关于其他集成和扩展。** LinnOS 提出的一个有趣问题是，为什么 SSD——具有复杂特性的设备——的延迟行为可以被块层用少数可观察的特征学习到。理解这一点可以帮助其他更高层，如 RAID、直接设备访问（SPDK）、用户/设备级文件系统或分布式存储采纳我们的概念。同样，在更低的层次，未来也有可能出现内置延迟推理能力的 SSD。尽管有人可能会说，设备已经完全了解其内部情况，不需要黑盒预测，但也可以认为，SSD 供应商可以在不同的内部架构中使用相同的机器学习方法。因此，他们不必每次修改内部硬件、逻辑和策略时都重新开发推理逻辑。或者，SSD 供应商可以采用结合了一些内部知识的“灰盒”学习。

**关于精度。** 快/慢推理能否转化为更精确的延迟推理，例如延迟范围（例如，2-4μs, 4-8μs, ...）、百分位桶（例如，p0-p10, ..., p90-p100），或具有高准确性的精确延迟？模型置换或其他机器学习技术能有所帮助吗？

# 7 致谢

我们感谢 Tom Anderson，我们的指导人，以及匿名审稿人给予的巨大反馈和有益评论。我们也感谢 Azure CSI 团队提供踪迹。本材料得到了 NSF（拨款号 CCF-2028427, CNS-1405959, CNS-1526304, CNS-1764039, 和 CNS-1823032）、ARO（W911NF1920321）、DOE（DESC00141950003）和芝加哥大学 CERES 中心的资助，以及来自 Dell EMC、Google 和 NetApp 的慷慨捐赠。

# 参考文献

 Cassandra - Speculative Execution for Reads / Eager Retries. https://issues.apache.org/jira/browse/ CASSANDRA-4705.
 Chameleon. https://www.chameleoncloud.org.
 Ethernet: The High Bandwidth Low- Latency Data Center Switching Fabric. http://www.forceel0networks. com/ whitepapers/pdf/F10_wp_Ethernet.pdf.
 GreyBeards on Storage. https://silvertonconsulting. com/gbos2/tag/tail- latency/.
 MongoDB - Basic Support for Operation Hedging in NetworkInterfaceTL. https://jira.mongodb.org/ browse/SERVER-45432.
 New GraphCore IPU BenchMarks. https://www. graphcore.ai/posts/ new- graphcore- ipu- benchmarks.
 Pareto Distribution. https://en.wikipedia.org/wiki/ Pareto_distribution.
 Rapid Read Protection in Cassandra 2.0.2. https://www. datastax.com/blog/2013/10/ rapid- read- protection- cassandra- 202.
 SNIA I/O Trace Data Files. http://iotta. snia. org/ traces.
 The Data Center Flash Storage Market Is Expected to Grow at a CAGR of Nearly About $17\%$ during 2018- 2024. https://prn.to/2a58q4L.
 The Evolution of Image Classification Explained. https:// stanford.edu/\~shervine/blog/ evolution- image- classification- explained.
 Tuning Speculative Retries to Fight Latency. https://www. youtube.com/watch?v=uRJSuQofJwQ, 2016.
... (以下参考文献列表保持原文)

# A 附件：工件

# A.1 摘要

我们组装了一个可在 Chameleon 云研究平台 上运行的可执行 LinnOS 工作流。这个自包含的工件包含了主要组件和分步说明。

# A.2 工件清单

- **程序：** LinnOS 及预处理脚本。
- **数据集：** 示例 I/O 踪迹。
- **运行环境：** Chameleon 的共享 Jupyter 实验环境。
- **硬件：** 至少有三个 SSD 的闪存阵列。
- **输出：** 用于 I/O 预测的训练模型和延迟 CDF 曲线。
- **实验：** LinnOS 工作流。
- **预期实验运行时间：** 几小时。
- **公开链接：** https://www.chameleoncloud.org/experiment/share/15?s=409ab137f20e4cd38ae3dd4e0d4bfa7c

# A.3 描述

# A.3.1 如何访问

访问上面提供的公开链接，点击“在 Chameleon 上启动”按钮（需要账户才能访问 Chameleon 资源），然后查看 Readme.txt 获取高层描述，LinnOS.ipynb 获取分步说明。

# A.3.2 硬件依赖

评估 LinnOS 需要一个至少有三个 SSD 的闪存阵列，这些由 Chameleon 测试平台的存储层次结构实例提供。

# A.3.3 数据集

工件包含一些示例 I/O 踪迹，用于工作流的测试目的。

# A.4 安装

分步安装说明可在工件中找到。

# A.5 评估和预期结果

成功运行后，工作流应产生一个训练好的模型、准确性结果以及 LinnOS 和基线的 I/O 延迟分布。请参阅工件中的 readme.txt 获取更多详情。