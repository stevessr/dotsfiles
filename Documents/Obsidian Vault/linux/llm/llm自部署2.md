---
cssclass: coffeebean-full-screen
---













| ‰∏äÊ∏∏‰æùËµñ  | È°πÁõÆÂêçÁß∞                                                     | ÁÆÄ‰ªã                                                         | ÊéàÊùÉÊñπÂºè           | Windows              | Linux                   | Mac(INTEL) | Mac(M)                                                       | docker | Web Browser       | jetson     | iOS                                                          | Android(È´òÈÄö)                                                | Android(ËÅîÂèëÁßë)                                              | special                                                      |
| --------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------ | -------------------- | ----------------------- | ---------- | ------------------------------------------------------------ | ------ | ----------------- | ---------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
|           | [llama.cpp](https://github.com/ggerganov/llama.cpp)          | LLM inference in C/C++                                       | MIT                | 10+                  | ‚úÖ                       | ‚úÖ          | ‚úÖ                                                            | ‚úÖ      |                   | ?          |                                                              |                                                              |                                                              | [Metal](https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md#metal-build)[BLAS](https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md#blas-build)[BLIS](https://github.com/ggerganov/llama.cpp/blob/master/docs/backend/BLIS.md)[SYCL](https://github.com/ggerganov/llama.cpp/blob/master/docs/backend/SYCL.md)[MUSA](https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md#musa)[CUDA](https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md#cuda)[HIP](https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md#hip)[Vulkan](https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md#vulkan)[CANN](https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md#cann) |
| llama.cpp | [Ollama](https://ollama.com/)                                | Get up and running with large language models.               | MIT                | 10+                  | ‚úÖ                       | ‚úÖ          | ‚úÖ                                                            | ‚úÖ      |                   | jetpack5/6 |                                                              |                                                              |                                                              |                                                              |
| llama.cpp | [LM Studio ](https://lmstudio.ai/)                           | Discover, download, and run local LLMs                       | Ôºü                 | 10+                  | x86_64                  |            | ‚úÖ                                                            |        |                   |            |                                                              |                                                              |                                                              |                                                              |
| llama.cpp | [Jan](https://jan.ai/)                                       | Open source ChatGPT-alternative that runs 100% offline       | AGPL-3.0           | x86_64               | x86_64                  | ‚úÖ          | ‚úÖ                                                            |        |                   |            |                                                              |                                                              |                                                              |                                                              |
| llama.cpp | [LocalAI](https://github.com/mudler/LocalAI)                 | ü§ñ The free, Open Source alternative to OpenAI, Claude and others. Self-hosted and local-first. Drop-in replacement for OpenAI, running on consumer-grade hardware. No GPU required. Runs gguf, transformers, diffusers and many more models architectures. Features: Generate Text, Audio, Video, Images, Voice Cloning, Distributed, P2P inference | MIT                |                      | ‚úÖ                       | ‚úÖ          | ‚úÖ                                                            | ‚úÖ      |                   |            |                                                              |                                                              |                                                              |                                                              |
|           | [mlc-llm: ](https://github.com/mlc-ai/mlc-llm)               | Universal LLM Deployment Engine with ML Compilation          | Apache-2.0         | ‚úÖVulkan, ROCm,Cuda   | ‚úÖVulkan, ROCm,Cuda      | ‚úÖ Metal    | ‚úÖ Metal (iGPU)                                               | ‚úÖ      | ‚úÖ WebGPU and WASM |            | ‚úÖ Metal on Apple A-series GPU                                | ‚úÖ OpenCL on Adreno GPU                                       | ‚úÖ OpenCL on Mali GPU                                         |                                                              |
|           | [pocketpal-ai:](https://github.com/a-ghorbani/pocketpal-ai)  | An app that brings language models directly to your phone.   | MIT                |                      |                         |            |                                                              |        |                   |            | ‚úÖ[PocketPal AI on the App¬†Store](https://apps.apple.com/us/app/pocketpal-ai/id6502579498) | ‚úÖ[PocketPal AI - Google Play ‰∏äÁöÑÂ∫îÁî®](https://play.google.com/store/apps/details?id=com.pocketpalai) | ‚úÖ[PocketPal AI - Google Play ‰∏äÁöÑÂ∫îÁî®](https://play.google.com/store/apps/details?id=com.pocketpalai) |                                                              |
|           | [vllm:](https://github.com/vllm-project/vllm)                | A high-throughput and memory-efficient inference and serving engine for LLMs | Apache-2.0         | ‚úÖ                    | ‚úÖ                       | ‚úÖ          | ‚úÖ                                                            | ‚úÖ      | ‚úÖ                 | ‚úÖ          | ‚úÖ                                                            | ‚úÖ                                                            | ‚úÖ                                                            | python                                                       |
| llama.cpp | [node-llama-cpp:](https://github.com/withcatai/node-llama-cpp) | Run AI models locally on your machine with node.js bindings for llama.cpp. Enforce a JSON schema on the model output on the generation level | MIT                | ‚úÖ                    | ‚úÖ                       | ‚úÖ          | ‚úÖ                                                            | ‚úÖ      | ‚úÖ                 | ‚úÖ          | ‚úÖ                                                            | ‚úÖ                                                            | ‚úÖ                                                            | node.js                                                      |
|           | [text-generation-inference](https://github.com/huggingface/text-generation-inference) | Large Language Model Text Generation Inference               | Apache-2.0         | ‚úÖ                    | ‚úÖ                       | ‚úÖ          | ‚úÖ                                                            | ‚úÖ      | ‚úÖ                 | ‚úÖ          | ‚úÖ                                                            | ‚úÖ                                                            | ‚úÖ                                                            | python<br>### Hardware support<br><br>[](https://github.com/huggingface/text-generation-inference#hardware-support)<br><br>- [Nvidia](https://github.com/huggingface/text-generation-inference/pkgs/container/text-generation-inference)<br>- [AMD](https://github.com/huggingface/text-generation-inference/pkgs/container/text-generation-inference)¬†(-rocm)<br>- [Inferentia](https://github.com/huggingface/optimum-neuron/tree/main/text-generation-inference)<br>- [Intel GPU](https://github.com/huggingface/text-generation-inference/pull/1475)<br>- [Gaudi](https://github.com/huggingface/tgi-gaudi)<br>- [Google TPU](https://huggingface.co/docs/optimum-tpu/howto/serving) |
| (‚äôÔπè‚äô)    | [anything-llm](https://github.com/Mintplex-Labs/anything-llm) | The all-in-one Desktop & Docker AI application with built-in RAG, AI agents, and more. | MIT                |                      |                         |            |                                                              |        |                   |            |                                                              |                                                              |                                                              |                                                              |
|           | [ortex.cpp:](https://github.com/janhq/cortex.cpp)            | Local AI API Platform                                        | Apache-2.0         | amd64                | amd64(debian offcial)   | ‚úÖ          | ‚úÖ                                                            |        |                   |            |                                                              |                                                              |                                                              |                                                              |
| llama.cpp | [edgen](https://github.com/edgenai/edgen)                    | ‚ö° Edgen: Local, private GenAI server alternative to OpenAI. No GPU required. Run AI models locally: LLMs (Llama2, Mistral, Mixtral...), Speech-to-text (whisper) and many others. | Apache-2.0         | ‚úÖ                    | ‚úÖ                       | ‚úÖ          | ‚úÖ                                                            |        |                   |            |                                                              |                                                              |                                                              | ÊñáÊ°£ÁΩëÁ´ôÁàÜÁÇ∏ÔºåÁñë‰ººÂºÉÂùë                                       |
| (‚äôÔπè‚äô)    | [ollama-app:](https://github.com/JHubi1/ollama-app)          | A modern and easy-to-use client for Ollama                   | Apache-2.0         |                      |                         |            |                                                              |        |                   |            |                                                              | ‚úÖ                                                            | ‚úÖ                                                            | Â∞±ÊòØ‰∏™ÂÆ¢Êà∑Á´Ø‚Ä¶‚Ä¶                                               |
| (‚äôÔπè‚äô)    | [LMPlayground:](https://github.com/andriydruk/LMPlayground)  | Language Model Playground                                    | MIT                |                      |                         |            |                                                              |        |                   |            |                                                              | ‚úÖ[LM Playground - Google Play ‰∏äÁöÑÂ∫îÁî®](https://play.google.com/store/apps/details?id=com.druk.lmplayground) | ‚úÖ[LM Playground - Google Play ‰∏äÁöÑÂ∫îÁî®](https://play.google.com/store/apps/details?id=com.druk.lmplayground) | Â∞±ÊòØ‰∏™ÂÆ¢Êà∑Á´Ø‚Ä¶‚Ä¶                                               |
|           | [mllm:](https://github.com/UbiquitousLearning/mLLM)          | Fast Multimodal LLM on Mobile Devices                        | MIT                |                      | ‚úÖ<br>Ëá™Â∑±ÁºñËØëÔºå‰∏∞Ë°£Ë∂≥È£ü |            |                                                              |        |                   |            |                                                              | ‚úÖNPU  <br>(INT8)/CPU(FP32/INT4)                              | ‚úÖCPU(FP32/INT4)                                              |                                                              |
|           | [Backyard AI \| Home](https://backyard.ai/)                  | Create Immersive  <br>AI-Powered¬†Characters                  | ?                  | ‚úÖ                    | ‚úÖ                       | ‚úÖ          | ‚úÖM1/M2/M3                                                    |        | ‚úÖ                 |            | ‚úÖ[Backyard.ai on the App¬†Store](https://apps.apple.com/us/app/backyard-ai/id6498968886) | [Backyard.ai - Google Play ‰∏äÁöÑÂ∫îÁî®](https://play.google.com/store/apps/details?id=ai.backyard)‚úÖ | ‚úÖ[Backyard.ai - Google Play ‰∏äÁöÑÂ∫îÁî®](https://play.google.com/store/apps/details?id=ai.backyard) |                                                              |
|           | [Jellybox](https://jellybox.com/)                            | Create, Generate, Share!                                     | ?                  | ‚úÖ(NVIDIA/AMD)        |                         |            | ‚úÖ                                                            |        |                   |            |                                                              |                                                              |                                                              | ÊîØÊåÅÁîüÊàêÂõæÁâá                                                 |
|           | [Msty](https://msty.app/)                                    | Using AI Models made Simple and Easy                         | Ôºü                 | ‚úÖX64(CPU/AMD/NVIDIA) | ‚úÖx64(CPU/AMD GPU/ROCM)  | ‚úÖ          | ‚úÖ                                                            |        |                   |            |                                                              |                                                              |                                                              | ÂèØ‰ª•ÈÄöËøáÊ∞™ÈáëÂèòÂº∫                                             |
|           | [Sanctum](https://sanctum.ai/)                               | Your Private, Local AI Assistant                             | Ôºü                 | ‚úÖX86_64              | Coming soon‚Ä¶‚Ä¶           | ‚úÖ          | ‚úÖ                                                            |        |                   |            |                                                              | ‚úÖ                                                            | ‚úÖ                                                            |                                                              |
|           | [Private AI - Google Play ‰∏äÁöÑÂ∫îÁî®](https://play.google.com/store/apps/details?id=us.valkon.privateai) |                                                              | Ôºü                 |                      |                         |            |                                                              |        |                   |            |                                                              |                                                              |                                                              |                                                              |
|           | [RecurseChat](https://recurse.chat/)                         |                                                              | ‰ªòË¥π‰ΩøÁî®ÔºåÂèØ‰ª•ÈÄÇÁî® |                      |                         | ‚úÖ          | ‚úÖ[RecurseChat on the Mac¬†App¬†Store](https://apps.apple.com/us/app/recursechat/id6476835702) |        |                   |            |                                                              |                                                              |                                                              |                                                              |
